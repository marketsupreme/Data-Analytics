{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"CS329E_HW7.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"6oDpcBLpgPlo"},"source":["## C S 329E HW 7\n","\n","# Naive Bayes \n","\n","## Michael Walters, Adrian Garcia - Group 19\n","\n","For this week's homework we are going explore one new classification technique:\n","\n","  - Naive Bayes\n","\n","We are reusing the version of the Melbourne housing data set from HW6, to predict the housing type as one of three possible categories:\n","\n","  - 'h' house\n","  - 'u' duplex\n","  - 't' townhouse\n","\n","In addition to building our own Naive Bayes classifier, we are going to compare the performace of our classifier to the [Gaussian Naive Bayes Classifier](https://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes) available in the scikit-learn library. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"7nYx9ECfgPlv","executionInfo":{"status":"ok","timestamp":1634524691493,"user_tz":300,"elapsed":623,"user":{"displayName":"Michael Walters","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiGWvqHL4weO5dM_zIK19_K6AJA7rS5rlNSG6ogeg=s64","userId":"18415248062348982019"}},"outputId":"d87781f9-d33c-4f80-fae4-1ead5f17593d"},"source":["# These are the libraries you will use for this assignment\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import time\n","import calendar\n","from sklearn.naive_bayes import GaussianNB # The only thing in scikit-learn you can use this assignment\n","\n","# Starting off loading a training set and setting a variable for the target column, \"Type\"\n","df_melb = pd.read_csv('https://gist.githubusercontent.com/yanyanzheng96/81b236aecee57f6cf65e60afd865d2bb/raw/56ddb53aa90c26ab1bdbfd0b8d8229c8d08ce45a/melb_data_train.csv')\n","target_col = 'Type'\n","df_melb"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Rooms</th>\n","      <th>Type</th>\n","      <th>Price</th>\n","      <th>Date</th>\n","      <th>Distance</th>\n","      <th>Postcode</th>\n","      <th>Bathroom</th>\n","      <th>Car</th>\n","      <th>Landsize</th>\n","      <th>BuildingArea</th>\n","      <th>YearBuilt</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2</td>\n","      <td>h</td>\n","      <td>399000</td>\n","      <td>7/5/16</td>\n","      <td>8.7</td>\n","      <td>3032</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>904</td>\n","      <td>53.0</td>\n","      <td>1985.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3</td>\n","      <td>h</td>\n","      <td>1241000</td>\n","      <td>28/08/2016</td>\n","      <td>13.9</td>\n","      <td>3165</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>643</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>u</td>\n","      <td>550000</td>\n","      <td>8/7/17</td>\n","      <td>3.0</td>\n","      <td>3067</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>1521</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>u</td>\n","      <td>691000</td>\n","      <td>24/06/2017</td>\n","      <td>8.4</td>\n","      <td>3072</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>170</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2</td>\n","      <td>u</td>\n","      <td>657500</td>\n","      <td>19/11/2016</td>\n","      <td>4.6</td>\n","      <td>3122</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>728</td>\n","      <td>73.0</td>\n","      <td>1965.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>495</th>\n","      <td>2</td>\n","      <td>t</td>\n","      <td>710000</td>\n","      <td>29/07/2017</td>\n","      <td>7.2</td>\n","      <td>3184</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>1980.0</td>\n","    </tr>\n","    <tr>\n","      <th>496</th>\n","      <td>2</td>\n","      <td>u</td>\n","      <td>446000</td>\n","      <td>16/04/2016</td>\n","      <td>8.0</td>\n","      <td>3040</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>497</th>\n","      <td>3</td>\n","      <td>h</td>\n","      <td>887000</td>\n","      <td>8/4/17</td>\n","      <td>9.2</td>\n","      <td>3058</td>\n","      <td>1</td>\n","      <td>2.0</td>\n","      <td>560</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>498</th>\n","      <td>3</td>\n","      <td>h</td>\n","      <td>1365000</td>\n","      <td>7/5/16</td>\n","      <td>8.0</td>\n","      <td>3040</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>754</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>499</th>\n","      <td>2</td>\n","      <td>u</td>\n","      <td>442000</td>\n","      <td>10/12/16</td>\n","      <td>4.2</td>\n","      <td>3031</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>3448</td>\n","      <td>71.0</td>\n","      <td>2010.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>500 rows × 11 columns</p>\n","</div>"],"text/plain":["     Rooms Type    Price        Date  ...  Car  Landsize  BuildingArea  YearBuilt\n","0        2    h   399000      7/5/16  ...  1.0       904          53.0     1985.0\n","1        3    h  1241000  28/08/2016  ...  1.0       643           NaN        NaN\n","2        2    u   550000      8/7/17  ...  1.0      1521           NaN        NaN\n","3        3    u   691000  24/06/2017  ...  1.0       170           NaN        NaN\n","4        2    u   657500  19/11/2016  ...  1.0       728          73.0     1965.0\n","..     ...  ...      ...         ...  ...  ...       ...           ...        ...\n","495      2    t   710000  29/07/2017  ...  1.0         0           NaN     1980.0\n","496      2    u   446000  16/04/2016  ...  1.0         0           NaN        NaN\n","497      3    h   887000      8/4/17  ...  2.0       560           NaN        NaN\n","498      3    h  1365000      7/5/16  ...  1.0       754           NaN        NaN\n","499      2    u   442000    10/12/16  ...  1.0      3448          71.0     2010.0\n","\n","[500 rows x 11 columns]"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"JmbMk15DgPlx"},"source":["## Q1 - Fix a column of data to be numeric\n","If we inspect our dataframe, `df_melb` using the `dtypes` method, we see that the column \"Date\" is an object.  However, we think this column might contain useful information so we want to convert it to [seconds since epoch](https://en.wikipedia.org/wiki/Unix_time). Use only the exiting imported libraries to create a new column \"unixtime\". Be careful, the date strings in the file might have some non-uniform formating that you have to fix first.  Print out the min and max epoch time to check your work.  Drop the original \"Date\" column. \n","\n","THESE ARE THE EXACT SAME INSTRUCTIONS FROM HW6! Please take this opportunity to reuse your code (if you got it right last time). "]},{"cell_type":"code","metadata":{"id":"izJv7-xrgPlz","executionInfo":{"status":"ok","timestamp":1634524691494,"user_tz":300,"elapsed":11,"user":{"displayName":"Michael Walters","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiGWvqHL4weO5dM_zIK19_K6AJA7rS5rlNSG6ogeg=s64","userId":"18415248062348982019"}}},"source":["# normalize date accepts the date string as shown in the df_melb 'Date' column,\n","# and returns a data in a standarized format\n","def standardize_date(d):\n","  (day, month, year) = d.split('/')\n","  if len(year) == 2:\n","    year = '20' + year\n","  return day + \"/\" + month + \"/\" + year"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"WvShLphJgPl0","executionInfo":{"status":"ok","timestamp":1634524691497,"user_tz":300,"elapsed":14,"user":{"displayName":"Michael Walters","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiGWvqHL4weO5dM_zIK19_K6AJA7rS5rlNSG6ogeg=s64","userId":"18415248062348982019"}},"outputId":"39eb88fc-cb7c-435d-a06a-a5059b01d600"},"source":["df_melb['Date'] = df_melb['Date'].apply( standardize_date )\n","df_melb['unixtime'] = df_melb['Date'].apply(lambda x: calendar.timegm(time.strptime(x, \"%d/%m/%Y\")))\n","df_melb = df_melb.drop(columns=\"Date\")\n","\n","df_melb"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Rooms</th>\n","      <th>Type</th>\n","      <th>Price</th>\n","      <th>Distance</th>\n","      <th>Postcode</th>\n","      <th>Bathroom</th>\n","      <th>Car</th>\n","      <th>Landsize</th>\n","      <th>BuildingArea</th>\n","      <th>YearBuilt</th>\n","      <th>unixtime</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2</td>\n","      <td>h</td>\n","      <td>399000</td>\n","      <td>8.7</td>\n","      <td>3032</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>904</td>\n","      <td>53.0</td>\n","      <td>1985.0</td>\n","      <td>1462579200</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3</td>\n","      <td>h</td>\n","      <td>1241000</td>\n","      <td>13.9</td>\n","      <td>3165</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>643</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1472342400</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>u</td>\n","      <td>550000</td>\n","      <td>3.0</td>\n","      <td>3067</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>1521</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1499472000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>u</td>\n","      <td>691000</td>\n","      <td>8.4</td>\n","      <td>3072</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>170</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1498262400</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2</td>\n","      <td>u</td>\n","      <td>657500</td>\n","      <td>4.6</td>\n","      <td>3122</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>728</td>\n","      <td>73.0</td>\n","      <td>1965.0</td>\n","      <td>1479513600</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>495</th>\n","      <td>2</td>\n","      <td>t</td>\n","      <td>710000</td>\n","      <td>7.2</td>\n","      <td>3184</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>1980.0</td>\n","      <td>1501286400</td>\n","    </tr>\n","    <tr>\n","      <th>496</th>\n","      <td>2</td>\n","      <td>u</td>\n","      <td>446000</td>\n","      <td>8.0</td>\n","      <td>3040</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1460764800</td>\n","    </tr>\n","    <tr>\n","      <th>497</th>\n","      <td>3</td>\n","      <td>h</td>\n","      <td>887000</td>\n","      <td>9.2</td>\n","      <td>3058</td>\n","      <td>1</td>\n","      <td>2.0</td>\n","      <td>560</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1491609600</td>\n","    </tr>\n","    <tr>\n","      <th>498</th>\n","      <td>3</td>\n","      <td>h</td>\n","      <td>1365000</td>\n","      <td>8.0</td>\n","      <td>3040</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>754</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1462579200</td>\n","    </tr>\n","    <tr>\n","      <th>499</th>\n","      <td>2</td>\n","      <td>u</td>\n","      <td>442000</td>\n","      <td>4.2</td>\n","      <td>3031</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>3448</td>\n","      <td>71.0</td>\n","      <td>2010.0</td>\n","      <td>1481328000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>500 rows × 11 columns</p>\n","</div>"],"text/plain":["     Rooms Type    Price  ...  BuildingArea  YearBuilt    unixtime\n","0        2    h   399000  ...          53.0     1985.0  1462579200\n","1        3    h  1241000  ...           NaN        NaN  1472342400\n","2        2    u   550000  ...           NaN        NaN  1499472000\n","3        3    u   691000  ...           NaN        NaN  1498262400\n","4        2    u   657500  ...          73.0     1965.0  1479513600\n","..     ...  ...      ...  ...           ...        ...         ...\n","495      2    t   710000  ...           NaN     1980.0  1501286400\n","496      2    u   446000  ...           NaN        NaN  1460764800\n","497      3    h   887000  ...           NaN        NaN  1491609600\n","498      3    h  1365000  ...           NaN        NaN  1462579200\n","499      2    u   442000  ...          71.0     2010.0  1481328000\n","\n","[500 rows x 11 columns]"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"h4p93lTTgPl0"},"source":["## Q2 Calculating the prior probabilities\n","Calculate the prior probabilities for each possible \"Type\" in `df_melb` and populate a dictionary, `dict_priors`, where the key is the possible \"Type\" values and the value is the prior probabilities. Show the dictionary. Do not hardcode the possible values of \"Type\".  Don't forget about [value counts](https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html). "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DarIs7MDgPl1","executionInfo":{"status":"ok","timestamp":1634524691501,"user_tz":300,"elapsed":16,"user":{"displayName":"Michael Walters","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiGWvqHL4weO5dM_zIK19_K6AJA7rS5rlNSG6ogeg=s64","userId":"18415248062348982019"}},"outputId":"c981aa1a-3d38-45b2-ebd3-be160005f961"},"source":["counts = df_melb['Type'].value_counts(normalize=True)\n","dict_priors = { k:v for (k,v) in zip(pd.unique(df_melb['Type']), counts.unique())}\n","dict_priors"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'h': 0.452, 't': 0.13, 'u': 0.418}"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"GZhTS6A1gPl2"},"source":["## Q3 Create a model for the distribution of all of the numeric attributes\n","For each class, and for each attribute calculate the sample mean and sample standard deviation.  You should store the model in a nested dictionary, `dict_nb_model`, such that `dict_nb_model['h']['Rooms']` is a tuple containing the mean and standard deviation for the target Type 'h' and the attribute 'Rooms'.  Show the model using the `display` function. You should ignore entries that are `NaN` in the mean and [standard deviation](https://pandas.pydata.org/docs/reference/api/pandas.Series.std.html) calculation. "]},{"cell_type":"code","metadata":{"id":"NmFfNAhDgPl4","executionInfo":{"status":"ok","timestamp":1634524691642,"user_tz":300,"elapsed":155,"user":{"displayName":"Michael Walters","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiGWvqHL4weO5dM_zIK19_K6AJA7rS5rlNSG6ogeg=s64","userId":"18415248062348982019"}}},"source":["dict_nb_model = dict()\n","columns = df_melb.columns.values\n","columns = np.delete(columns, np.where(columns == 'Type'))\n","\n","for target in dict_priors.keys():\n","  df_train = df_melb[df_melb['Type'] == target]\n","  dict_nb_model[target] = {}\n","  for column in columns:\n","    average = df_train[column].mean(skipna=True)\n","    stdD = df_train[column].std(skipna=True)\n","    dict_nb_model[target][column] = (average, stdD)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":568},"id":"j4yVXHjpgPl5","executionInfo":{"status":"ok","timestamp":1634524691642,"user_tz":300,"elapsed":10,"user":{"displayName":"Michael Walters","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiGWvqHL4weO5dM_zIK19_K6AJA7rS5rlNSG6ogeg=s64","userId":"18415248062348982019"}},"outputId":"0111d992-c6f7-4937-e6eb-5f7d868cece3"},"source":["display(dict_nb_model)"],"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":["{'h': {'Bathroom': (1.5619469026548674, 0.6720871086493075),\n","  'BuildingArea': (156.24339622641511, 54.62662837301434),\n","  'Car': (1.7777777777777777, 0.932759177140425),\n","  'Distance': (12.086725663716809, 7.397501132737295),\n","  'Landsize': (932.9646017699115, 3830.7934157687164),\n","  'Postcode': (3103.8982300884954, 98.35750345419703),\n","  'Price': (1189022.3451327435, 586296.5794417895),\n","  'Rooms': (3.269911504424779, 0.7258264201127756),\n","  'YearBuilt': (1954.900826446281, 32.461876347154686),\n","  'unixtime': (1485717578.761062, 13838562.050601462)},\n"," 't': {'Bathroom': (1.8461538461538463, 0.565430401076506),\n","  'BuildingArea': (138.66666666666666, 53.498637054290135),\n","  'Car': (1.6923076923076923, 0.5280588545286915),\n","  'Distance': (10.766153846153845, 4.870455475462387),\n","  'Landsize': (268.18461538461537, 276.57700624711265),\n","  'Postcode': (3121.6153846153848, 100.01588816090862),\n","  'Price': (1000169.2307692308, 421822.5363389935),\n","  'Rooms': (2.9076923076923076, 0.6052653582075831),\n","  'YearBuilt': (1997.0227272727273, 16.99177453038181),\n","  'unixtime': (1486525292.3076923, 12640127.60987191)},\n"," 'u': {'Bathroom': (1.1818181818181819, 0.42228151548662185),\n","  'BuildingArea': (83.85585585585585, 45.959438015166604),\n","  'Car': (1.1483253588516746, 0.47231993860296956),\n","  'Distance': (8.760287081339715, 5.609778714430755),\n","  'Landsize': (436.23444976076553, 1394.3403794653254),\n","  'Postcode': (3120.4545454545455, 87.18475679946482),\n","  'Price': (634207.1770334928, 217947.32866736987),\n","  'Rooms': (2.0430622009569377, 0.5908453859944267),\n","  'YearBuilt': (1976.451388888889, 24.557291330642666),\n","  'unixtime': (1484176719.617225, 13494566.111289721)}}"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"krA5Qx2ZgPl5"},"source":["## Q4 Write a function that calculates the probability of a Gaussian\n","Given the mean ($\\mu$), standard deviation ($\\sigma$), and a observed point, `x`, return the probability.  \n","Use the formula $p(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}$ ([wiki](https://en.wikipedia.org/wiki/Normal_distribution)).  You should use [numpy's exp](https://numpy.org/doc/stable/reference/generated/numpy.exp.html) function in your solution. "]},{"cell_type":"code","metadata":{"id":"QWVsD-QYgPl6","executionInfo":{"status":"ok","timestamp":1634524691643,"user_tz":300,"elapsed":10,"user":{"displayName":"Michael Walters","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiGWvqHL4weO5dM_zIK19_K6AJA7rS5rlNSG6ogeg=s64","userId":"18415248062348982019"}}},"source":["def get_p( mu, sigma, x):\n","  pi = 2*np.pi\n","  denom = sigma*np.sqrt(pi)\n","  frac = 1/denom\n","  zsq = ((x-mu)/(sigma))*((x-mu)/(sigma))\n","  expo = np.exp((-1/2)*zsq)\n","  probability = frac*expo\n","\n","  return probability"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LgfKgwUogPl7","executionInfo":{"status":"ok","timestamp":1634524691645,"user_tz":300,"elapsed":11,"user":{"displayName":"Michael Walters","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiGWvqHL4weO5dM_zIK19_K6AJA7rS5rlNSG6ogeg=s64","userId":"18415248062348982019"}},"outputId":"b9365cff-6a8d-4720-e610-9e5cf633961b"},"source":["# Test it\n","p = get_p( 0, 2, 0.5)\n","observation = df_melb.iloc[[4]]\n","observation['Rooms'].item()"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"pNVvRUgWgPl7"},"source":["## Q5 Write the Naive Bayes classifier function\n","The Naive Bayes classifier function, `nb_class`, should take as a parameter the prior probability dictionary. `dict_priors`, the dictionary containing all of the gaussian distribution information for each attribue, `dict_nb_model`, and a single observation row (a series generated from iterrows) of the test dataframe. It should return a single target classification. For this problem, all of our attributes are numeric and modeled as Gaussians, so we don't worry about categorical data. Make sure to skip attributes that do not have a value in the observation.  Do not hardcode the possible classification types. "]},{"cell_type":"code","metadata":{"id":"lyVOH-2RgPl7","executionInfo":{"status":"ok","timestamp":1634524691645,"user_tz":300,"elapsed":8,"user":{"displayName":"Michael Walters","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiGWvqHL4weO5dM_zIK19_K6AJA7rS5rlNSG6ogeg=s64","userId":"18415248062348982019"}}},"source":["def nb_class( dict_priors, dict_nb_model, observation):\n","  columns = observation.columns.values\n","  columns = np.delete(columns, np.where(columns == 'Type'))\n","\n","  bayesClass = {}\n","  for key in dict_nb_model.keys():\n","    prob = dict_priors[key]\n","    for column in columns:\n","      summary = dict_nb_model[key][column]\n","      p = get_p(summary[0],summary[1],observation[column].item())\n","      prob *= p\n","    bayesClass[key] = prob\n","\n","  return max(bayesClass, key = bayesClass.get)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"46My_wHlgPl8"},"source":["## Q6 Calculate the accuracy using Naive Bayes classifier function on the test set\n","Load the test set from file, convert date to unix time and drop the date column, classify each row using your `nb_class`, and then show the accuracy on the test set. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"5-SuEKrggPl8","executionInfo":{"status":"ok","timestamp":1634524691786,"user_tz":300,"elapsed":149,"user":{"displayName":"Michael Walters","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiGWvqHL4weO5dM_zIK19_K6AJA7rS5rlNSG6ogeg=s64","userId":"18415248062348982019"}},"outputId":"b9348ea0-cd8e-4be5-c7d7-74677196d004"},"source":["df_test = pd.read_csv('https://gist.githubusercontent.com/yanyanzheng96/c3d53303cebbd986b166591d19254bac/raw/94eb3b2d500d5f7bbc0441a8419cd855349d5d8e/melb_data_test.csv')\n","df_test['Date'] = df_test['Date'].apply( standardize_date )\n","df_test['unixtime'] = df_test['Date'].apply(lambda x: calendar.timegm(time.strptime(x, \"%d/%m/%Y\")))\n","df_test = df_test.drop(columns=\"Date\")\n","df_test"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Rooms</th>\n","      <th>Type</th>\n","      <th>Price</th>\n","      <th>Distance</th>\n","      <th>Postcode</th>\n","      <th>Bathroom</th>\n","      <th>Car</th>\n","      <th>Landsize</th>\n","      <th>BuildingArea</th>\n","      <th>YearBuilt</th>\n","      <th>unixtime</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3</td>\n","      <td>h</td>\n","      <td>1116000</td>\n","      <td>17.9</td>\n","      <td>3192</td>\n","      <td>1</td>\n","      <td>2.0</td>\n","      <td>610</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1498867200</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3</td>\n","      <td>h</td>\n","      <td>2030000</td>\n","      <td>11.2</td>\n","      <td>3186</td>\n","      <td>2</td>\n","      <td>2.0</td>\n","      <td>366</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1472342400</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>h</td>\n","      <td>1480000</td>\n","      <td>10.7</td>\n","      <td>3187</td>\n","      <td>2</td>\n","      <td>2.0</td>\n","      <td>697</td>\n","      <td>143.0</td>\n","      <td>1925.0</td>\n","      <td>1478476800</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>u</td>\n","      <td>1203500</td>\n","      <td>12.3</td>\n","      <td>3166</td>\n","      <td>2</td>\n","      <td>2.0</td>\n","      <td>311</td>\n","      <td>127.0</td>\n","      <td>2000.0</td>\n","      <td>1495843200</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3</td>\n","      <td>h</td>\n","      <td>540000</td>\n","      <td>14.7</td>\n","      <td>3030</td>\n","      <td>2</td>\n","      <td>2.0</td>\n","      <td>353</td>\n","      <td>135.0</td>\n","      <td>2011.0</td>\n","      <td>1504396800</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>3</td>\n","      <td>u</td>\n","      <td>882000</td>\n","      <td>5.6</td>\n","      <td>3101</td>\n","      <td>2</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","      <td>141.0</td>\n","      <td>1950.0</td>\n","      <td>1464998400</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>1</td>\n","      <td>u</td>\n","      <td>340000</td>\n","      <td>2.3</td>\n","      <td>3051</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1480723200</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>3</td>\n","      <td>h</td>\n","      <td>1006000</td>\n","      <td>8.8</td>\n","      <td>3081</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>758</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1502496000</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>4</td>\n","      <td>h</td>\n","      <td>1975000</td>\n","      <td>6.4</td>\n","      <td>3078</td>\n","      <td>2</td>\n","      <td>2.0</td>\n","      <td>600</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1495238400</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>3</td>\n","      <td>h</td>\n","      <td>670000</td>\n","      <td>12.4</td>\n","      <td>3060</td>\n","      <td>1</td>\n","      <td>3.0</td>\n","      <td>587</td>\n","      <td>136.0</td>\n","      <td>NaN</td>\n","      <td>1480723200</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100 rows × 11 columns</p>\n","</div>"],"text/plain":["    Rooms Type    Price  ...  BuildingArea  YearBuilt    unixtime\n","0       3    h  1116000  ...           NaN        NaN  1498867200\n","1       3    h  2030000  ...           NaN        NaN  1472342400\n","2       3    h  1480000  ...         143.0     1925.0  1478476800\n","3       3    u  1203500  ...         127.0     2000.0  1495843200\n","4       3    h   540000  ...         135.0     2011.0  1504396800\n","..    ...  ...      ...  ...           ...        ...         ...\n","95      3    u   882000  ...         141.0     1950.0  1464998400\n","96      1    u   340000  ...           NaN        NaN  1480723200\n","97      3    h  1006000  ...           NaN        NaN  1502496000\n","98      4    h  1975000  ...           NaN        NaN  1495238400\n","99      3    h   670000  ...         136.0        NaN  1480723200\n","\n","[100 rows x 11 columns]"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"QyU8-JfSgPl-","executionInfo":{"status":"ok","timestamp":1634524691929,"user_tz":300,"elapsed":147,"user":{"displayName":"Michael Walters","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiGWvqHL4weO5dM_zIK19_K6AJA7rS5rlNSG6ogeg=s64","userId":"18415248062348982019"}}},"source":["predictions = []\n","\n","df_tst = df_test.copy()\n","df_tst = df_tst.dropna().reset_index(drop=True)\n","for (indx,row) in df_tst.iterrows():\n","  observation = df_tst.iloc[[indx]]\n","  predictions.append(nb_class( dict_priors, dict_nb_model, observation))"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"pLSZOiZkgPl-","executionInfo":{"status":"ok","timestamp":1634524691931,"user_tz":300,"elapsed":15,"user":{"displayName":"Michael Walters","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiGWvqHL4weO5dM_zIK19_K6AJA7rS5rlNSG6ogeg=s64","userId":"18415248062348982019"}}},"source":["def get_acc(a, b):\n","    a = a.to_numpy()\n","    count = 0\n","\n","    for i in range(len(a)):\n","        if a[i] == b[i]:\n","            count += 1\n","    accuracy = count/len(a)\n","    return accuracy\n","\n","acc = get_acc(df_tst['Type'], predictions)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TNunRSXsgPl_","executionInfo":{"status":"ok","timestamp":1634524691932,"user_tz":300,"elapsed":15,"user":{"displayName":"Michael Walters","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiGWvqHL4weO5dM_zIK19_K6AJA7rS5rlNSG6ogeg=s64","userId":"18415248062348982019"}},"outputId":"4a05bdc3-9051-42eb-a186-0558bb72273f"},"source":["print('Accuracy is {:.2f}%'.format(acc*100))"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy is 61.36%\n"]}]},{"cell_type":"markdown","metadata":{"id":"cE6PldZzgPl_"},"source":["## Use scikit-learn to do the same thing!\n","\n","Now we understand the inner workings of the Naive Bayes algorithm, let's compare our results to [scikit-learn's Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html) implementation. Use the [GaussianNB](https://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes) to train using the `df_melb`dataframe and test using the `df_test` dataframe. Remember to split `df_melb` into a `df_X` with the numerical attributes, and a `s_y` with the target column. On the `df_melb` frame you will have to fill the empty attributes via imputation since the scikit-learn library can not handle missing values.  Use the same method you used in the last homework (filling the training data with the mean of the non-nan values). \n","\n","Answer the following in a markdown cell: do you think imputation hurt or helped the classifier?"]},{"cell_type":"code","metadata":{"id":"AA6kqotagPl_","executionInfo":{"status":"ok","timestamp":1634524691933,"user_tz":300,"elapsed":13,"user":{"displayName":"Michael Walters","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiGWvqHL4weO5dM_zIK19_K6AJA7rS5rlNSG6ogeg=s64","userId":"18415248062348982019"}}},"source":["# Imputation training\n","df_train = df_melb.copy()\n","dict_imputation = dict()\n","for col in df_train.columns:\n","  if col != target_col:\n","    df_train[col].fillna(df_train[col].mean(), inplace = True)\n","    dict_imputation[col] = df_train[col].mean()\n","  continue\n","        \n","# Imputation - apply on the test data\n","for col in df_test.columns:\n","  if col != target_col:\n","    df_test[col].fillna(dict_imputation[col], inplace = True)\n","  continue\n","\n","# Seperate the attributes from the target_col\n","s_y = df_train['Type']\n","df_X = df_train.drop(columns='Type')\n","df_test2 = df_test.drop(columns='Type')"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"phFnYFAUgPmA","executionInfo":{"status":"ok","timestamp":1634524691934,"user_tz":300,"elapsed":14,"user":{"displayName":"Michael Walters","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiGWvqHL4weO5dM_zIK19_K6AJA7rS5rlNSG6ogeg=s64","userId":"18415248062348982019"}},"outputId":"1deb7a5f-6b6d-4aa7-b549-07a10a852bfe"},"source":["gnb = GaussianNB()\n","prediction = gnb.fit(df_X, s_y).predict(df_test2)\n","prediction"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['t', 'h', 't', 't', 'u', 'u', 't', 'u', 't', 'u', 'u', 't', 'u',\n","       'u', 'u', 't', 'h', 'h', 't', 'u', 't', 't', 'u', 'h', 't', 't',\n","       'u', 'h', 'h', 't', 't', 't', 't', 'u', 'h', 'u', 'u', 'h', 't',\n","       't', 't', 'u', 'u', 'u', 'h', 'h', 'u', 'u', 't', 'u', 't', 't',\n","       't', 'u', 'u', 'u', 't', 't', 'u', 'u', 't', 'u', 'u', 'u', 't',\n","       'h', 't', 'u', 'u', 'u', 'u', 't', 'u', 't', 'u', 't', 'u', 'u',\n","       'u', 't', 'u', 'h', 'u', 'u', 'u', 'u', 't', 't', 'u', 'u', 'u',\n","       'u', 'u', 'h', 'u', 'u', 'u', 't', 'h', 'u'], dtype='<U1')"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GyfIgIM0gPmA","executionInfo":{"status":"ok","timestamp":1634524709123,"user_tz":300,"elapsed":139,"user":{"displayName":"Michael Walters","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiGWvqHL4weO5dM_zIK19_K6AJA7rS5rlNSG6ogeg=s64","userId":"18415248062348982019"}},"outputId":"ad065baf-d37f-4511-dd09-c396047eb916"},"source":["acc = get_acc(df_test['Type'],prediction)\n","print('Accuracy is {:.2f}%'.format(acc*100))"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy is 37.00%\n"]}]},{"cell_type":"markdown","metadata":{"id":"v3B-0yYlgPmB"},"source":["## ANSWER TO \"do you think imputation hurt or helped the classifier?\" \n","\n","I think imputation did in fact hurt the classifier. It is possible a more rigid method of imputation aside from using the mean would be more effective, perhaps linear regression."]}]}